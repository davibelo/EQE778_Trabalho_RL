# -*- coding: utf-8 -*-
"""Problem_1_CSTR_custom_reward_FranciscoDaviBeloRodrigues.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WGEmqlEpzO4VUyo6DMJE5uUwJO1Ajug9
"""
from dataclasses import dataclass
import numpy as np
import torch
import matplotlib.pyplot as plt
import gymnasium as gym
import torch.nn.functional as F
import copy
from stable_baselines3 import PPO,SAC,DDPG,TD3
import pcgym
import random

# Define environment
T = 26
nsteps = 150

# Global seed for reproducibility
seed = 1490

"""# Control Club Challenge (PSE)
## 1. Control Problem

The environment used in the this problem is a Continuously Stirred Tank Reactor (CSTR) performing an exothermic reaction (A â†’ B). The controlled variable will be the concentration of species A by manipulating the temperature of the cooling jacket. A diagram of the environment is given below.

![CSTR_PFD.png](data:image/png;base64,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)

### Problem Definition

You must design a control policy which maximises the reward defined by the square error between the state $C_A$ and its set point $C_{A,SP}$ for the environment below. There is also a disturbance to the input feed concentration $C_{A,IN}$ which happens around 9 minutes into the episode, returning again to the original value at ~18 minutes.

## 2. Challenge Environment

First define the challenge environment
"""

from pcgym import make_env
# from callback import LearningCurveCallback
import numpy as np
from stable_baselines3 import PPO, DDPG, SAC

SP = {
    'Ca': [0.85 for i in range(int(nsteps/3))] + [0.9 for i in range(int(nsteps/3))]+ [0.87 for i in range(int(nsteps/3))],
}

disturbance = {'Caf': np.repeat([1, 1.05, 1], [nsteps//3, nsteps//3, nsteps//3])}

action_space = {
    'low': np.array([295]),
    'high':np.array([302])
}
#Continuous box observation space  Specifications (Ca,T,Ca_SP)
observation_space = {
    'low' : np.array([0.7,300,0.8]),
    'high' : np.array([1,350,0.9])
}

r_scale = {'Ca':1e3}

disturbance_space ={
  'low': np.array([1]),
  'high': np.array([1.05])
}

########################################################################################################################################################################

# Examples of reward functions that were given on the paper : https://arxiv.org/pdf/2410.22093 (Chapter 7 Reward Functions)

# Custom reward:
# Negative squared error (39): will penalize the agent for being far from the setpoint.
# However, it will penalize the agent more heavily compared to the absolute error as it moves away from the set point.

def r_squared(self,x,u,con):
    Sp_i = 0
    cost = 0

    for k in self.env_params["SP"]:
        i = self.model.info()["states"].index(k)
        SP = self.SP[k]

        o_space_low = self.env_params["o_space"]["low"][i]
        o_space_high = self.env_params["o_space"]["high"][i]

        x_normalized = (x[i] - o_space_low) / (o_space_high - o_space_low)
        setpoint_normalized = (SP - o_space_low) / (o_space_high - o_space_low)

        cost += (np.sum(x_normalized - setpoint_normalized[self.t]) ** 2)

        Sp_i += 1

    r = -cost

    return r

###########################################################################################

# Define environment
env_params = {
  'N': nsteps,
  'tsim': T,
  'SP': SP,
  'o_space': observation_space,
  'a_space': action_space,
  'x0': np.array([0.8,330,0.8]), # initial conditions of the state vector [Ca, T, Ca_SP]
  'r_scale': {'Ca':1e3},
  'model': 'cstr',
  'normalise_a': True,
  'normalise_o': True,
  'noise': True,
  'integration_method': 'casadi',
  'noise_percentage': 0.001,
  'disturbance_bounds': disturbance_space,
  'disturbances': disturbance,
  'custom_reward': r_squared # only the squared reward function
}
env = make_env(env_params)

"""## 3. Example Policy and Rollout

An example policy which returns an action from an uniform distribution between the upper and lower control bounds.


$\pi = \mathcal U(u_{lb},u_{ub})$
"""

class policy:
    def predict(self,deterministic = False):
        return random.uniform(-1, 1), 0

"""Rollout random policy using the `plot_rollout` method which plots the rollout of the policy, and returns the rollout data and evaluator class. The policy can be plotted with an oracle (MPC with perfect model) which gives you a indication about the optimal control trajectory may look like. Note you can also intract with the environment like you would with any OpenAI gym :)."""

import sys
import os

# Suppress output for cleaner execution
# original_stdout = sys.stdout
# original_stderr = sys.stderr
# sys.stdout = open(os.devnull, 'w')
# sys.stderr = open(os.devnull, 'w')

# Run
data, eval = env.plot_rollout({'Random policy':policy}, reps = 5, oracle = True, dist_reward=True,)

# Re-enable output
# sys.stdout = original_stdout
# sys.stderr = original_stderr

"""## 3. Your Implementation!
Some helpful resourses:
  - [Stable Baselines](https://stable-baselines3.readthedocs.io/en/master/) - Off the shelf implementations of common RL algorithms
 -  [Minimal RL ](https://github.com/seungeunrho/minimalRL) - Minimal versions of common RL algorithms (useful for understanding how to translate the algorithms into code)
 -  [Spinning Up in DRL](https://spinningup.openai.com/en/latest/index.html) - More implememtations from OpenAI
 - [Gymnasium](https://gymnasium.farama.org/) - The environment framework which pc-gym is built on
 - [pc-gym documentation](https://maximilianb2.github.io/pc-gym/) - The docs!
"""

# Implement your RL algorithm  here! (We've Implemented a simple proportional controller to get you started)
class your_policy:
    def __init__(self) -> None:
        pass
    def predict(s,deterministic = False):
        kp = 0.75
        u = -1*kp*(s[2] - s[0])
        return u, s

import sys
import os

# Run
data, eval = env.plot_rollout({'Random policy': your_policy}, reps = 5, oracle = True, dist_reward=True,)

"""# Hyper parameter optimization with Optuna

import optuna
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.evaluation import evaluate_policy
import numpy as np
import os
from contextlib import contextmanager

# Suppress output for evaluate_policy and model.learn
@contextmanager
def suppress_output():
    with open(os.devnull, 'w') as devnull:
        old_stdout = sys.stdout
        old_stderr = sys.stderr
        try:
            sys.stdout = devnull
            sys.stderr = devnull
            yield
        finally:
            sys.stdout = old_stdout
            sys.stderr = old_stderr

def linear_schedule(initial_lr, final_lr):
    return lambda progress_remaining: progress_remaining * (initial_lr - final_lr) + final_lr

def optimize_ddpg(trial):
    # Define the hyperparameter search space
    buffer_size = trial.suggest_int("buffer_size", 500, 2000, step=500)
    learning_starts = trial.suggest_int("learning_starts", 5, 20, step=1)
    batch_size = trial.suggest_categorical("batch_size", [32, 64, 128, 256])
    tau = trial.suggest_float("tau", 0.001, 0.01, step=0.001)
    gamma = trial.suggest_float("gamma", 0.95, 0.9999)
    sigma = trial.suggest_float("sigma", 0.1, 0.5, step=0.05)
    theta = trial.suggest_float("theta", 0.1, 0.4, step=0.05)
    dt = trial.suggest_float("dt", 0.001, 0.05)
    initial_lr = trial.suggest_float("initial_lr", 0.001, 0.003, step=0.0001)
    final_lr = trial.suggest_float("final_lr", 0.0005, 0.001, step=0.0001)
    nsteps_learning = trial.suggest_int("nsteps_learning", 10000, 50000, step=5000)

    # Define the action noise
    n_actions = env.action_space.shape[-1]
    action_noise = OrnsteinUhlenbeckActionNoise(
        mean=np.zeros(n_actions),
        sigma=sigma * np.ones(n_actions),
        theta=theta,
        dt=dt
    )

    # Save path for evaluation
    save_path = f"/content/best_model_trial_{trial.number}/"
    os.makedirs(save_path, exist_ok=True)

    eval_callback = EvalCallback(
        eval_env=env,
        best_model_save_path=save_path,
        log_path=save_path,
        eval_freq=nsteps_learning // 10,  # Reduced frequency for less evaluations
        deterministic=True,
        render=False
    )

    # Instantiate the DDPG model
    model = DDPG(
        "MlpPolicy",
        env,
        verbose=0,
        learning_rate=linear_schedule(initial_lr, final_lr),
        action_noise=action_noise,
        buffer_size=buffer_size,
        learning_starts=learning_starts,
        batch_size=batch_size,
        tau=tau,
        gamma=gamma,
    )

    # Train the model
    with suppress_output():
        model.learn(total_timesteps=nsteps_learning, callback=eval_callback)

    # Evaluate the model
    with suppress_output():
        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)

    # Return the mean reward (Optuna maximizes the objective)
    return mean_reward

# Create an Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(optimize_ddpg, n_trials=100)

# Print the best hyperparameters
print("Best hyperparameters:", study.best_params)

# RL training with DDPG
"""

# Example of how to train an RL policy with the DDPG algorithm using the Stable Baselines 3 package
from stable_baselines3 import DDPG
from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise
from stable_baselines3.common.callbacks import EvalCallback
import os
import numpy as np

# We got from the Optuna optimisation:
# {buffer_size': 1500, 'learning_starts': 16, 'batch_size': 128, 'tau': 0.006, 'gamma': 0.961239294568761,
# 'sigma': 0.25, 'theta': 0.4, 'dt': 0.028517525751839627,
# 'initial_lr': 0.0011, 'final_lr': 0.0007, 'nsteps_learning': 35000}.

# Global timesteps (total number of steps):
nsteps_learning = 35000

# Define the action space bounds. You can get the action space bounds from the environment
n_actions = env.action_space.shape[-1]

# DDPG is an off-policy, deterministic algorithm, which means the policy outputs the same action
# for a given state during training unless noise is explicitly added.
# Action noise encourages exploration, helping the agent learn better policies.
action_noise = OrnsteinUhlenbeckActionNoise(
    mean=np.zeros(n_actions),
    sigma=0.25 * np.ones(n_actions),  # Adjust exploration magnitude
    theta=0.4,                       # Adjust reversion speed
    dt=0.028517525751839627           # Fine-tune time steps
)

# Define a learning rate decay schedule (ex. linear)
def linear_schedule(progress):
    # Progress goes from 1 (start) to 0 (end)
    initial_lr = 0.0011
    final_lr =  0.0007
    return progress * (initial_lr - final_lr) + final_lr

# Custom parameters for DDPG
custom_params = {
    "buffer_size": 1500,
    "learning_starts": 16,                 # When to start learning
    "batch_size": 128,
    "tau": 0.006,                         # Target network update factor
    "gamma": 0.961239294568761            # Discount factor for rewards
}

# Directory for saving the best model
save_path = "/content/best_model/"
os.makedirs(save_path, exist_ok=True)

# Callback for saving the best model
eval_callback = EvalCallback(
    eval_env=env,        # Use the same environment for evaluation
    best_model_save_path=save_path,
    log_path=save_path,
    eval_freq=100,       # Evaluate every 100 steps
    deterministic=True,  # Use deterministic policy during evaluation
    render=False         # Disable rendering during evaluation
)

# Instantiate the DDPG model
DDPG_CSTR = DDPG(
    "MlpPolicy",
    env,
    verbose=1,
    learning_rate=linear_schedule,  # Pass the schedule function here
    seed=seed,                      # Use a fixed seed for reproducibility
    action_noise=action_noise,      # Add Ornstein-Uhlenbeck action noise
    buffer_size=custom_params["buffer_size"],
    learning_starts=custom_params["learning_starts"],
    batch_size=custom_params["batch_size"],
    tau=custom_params["tau"],
    gamma=custom_params["gamma"],
)

# Train the model with the evaluation callback
DDPG_CSTR.learn(total_timesteps=int(nsteps_learning), callback=eval_callback)

# Print the path where the best model was saved
print(f"Best model saved at: {save_path}/best_model.zip")

"""# 4. Results"""

# Load the best model after training
best_model = DDPG.load(f"{save_path}/best_model.zip")

evaluator, data = env.plot_rollout({'DDPG': best_model}, reps = 10, oracle = True, dist_reward=True,)

data

"""### Your score!"""

res = data['DDPG']['r'].mean()
print(f"Your final score is: {res:.6f} (larger the better).")

data['DDPG']['Ca'].mean()

data['DDPG'].keys()